---
layout: page
title: resources
permalink: /resources/
description: some helpful pointers
nav: true
nav_order: 5
horizontal: false
---
**General research tips and blogs.** 
* [Andrej Karpathy](https://karpathy.ai/)'s blog, especially his [survival guide to a PhD](https://karpathy.github.io/2016/09/07/phd/). 
* Richard Hamming's talk on [You and Your Research](https://www.inf.ed.ac.uk/teaching/courses/inf1/fp/lectures/hamming.pdf). [Video](https://www.youtube.com/watch?v=a1zDuOPkMSw&ab_channel=securitylectures).
* [Lessons learned from 1K rejected research papers](https://www.cs.cmu.edu/~christos/MetaPaper/faloutsos_MetaPaper_1KR.pdf) by Christos Faloutsos. 
* On writing good research code: [Harvard](https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit#heading=h.fmfv56no579j), [reddit](https://www.reddit.com/r/MachineLearning/comments/g133a3/r_what_is_your_ml_research_workflow/), and [industry research](https://www.moderndescartes.com/essays/research_code/) perspectives. 
* Use [screens](https://linuxize.com/post/how-to-use-linux-screen/) to detach from your lab's cloud computing cluster overnight. Oh, and use [Termius](https://termius.com/) to make remote development and data sharing smoothly. Free with [Github Student Developer Pack](https://education.github.com/pack)!
* Concerns about AI functionalities scraping your code or thoughts? I am no expert, but here's a [blog](https://paulsorensen.io/github-copilot-vscode-privacy/) I found specifically for VSCode and the Co-pilot integration. 
* A nice [guide](https://www.alexpghayes.com/post/2023-12-20_getting-statistics-help/) by Alex Hayes on important questions to address *prior to data-driven collaborations*, both for the statistician and domain expert seeking assistance. 
* For applied statistics insights, check out [Ben Recht](https://www.argmin.net/) and [Andrew Gelman](https://statmodeling.stat.columbia.edu/)'s blogs. 
* My two favorite bloggers are [Matt Might](https://matt.might.net/) and [Andrej Karpathy](https://karpathy.ai/). I also regularly refer to the [Kernel Magazine](https://www.kernelmag.io/) for diversified perspectives on how tech impacts our society and proposals to leverage technology for a better future. When it comes to digesting AI news, I personally find [Import AI](https://jack-clark.net/) and [Interconnects](https://www.interconnects.ai/) helpful. 

**Early researchers and grad school applicants!!!**

Check out my blog post for [getting involved with ML at Rice](../blog/2023/ml-at-rice/) and this [video series](https://www.youtube.com/@FAILSharing) on failures experienced by leading scholars, which deeply inspired me. 


You should also be aware of
* [REU](https://www.nsf.gov/crssprgm/reu/) programs, which are funded summer research program for U.S. students. Many target those with little prior experience and from underrepresented backgrounds. 
* [Barry Goldwater Scholarship](https://goldwaterscholarship.gov/) for sophomore, juniors. Schools usually pre-select candidates. 
* [Cornell, Maryland, and Max Planck Pre-Doctoral Summer School](https://cmmrs.mpi-sws.org/) for those debating to pursue CS research. You get a fun, fully-funded trip to Europe as well! 
* [NSF GRFP](https://www.nsfgrfp.org/), [Hertz](https://www.hertzfoundation.org/hertz-fellows/), and [DOE CSGF](https://www.krellinst.org/csgf/) fellowships for Ph.D. applicants. Apply as these can make you an incredibly competitive applicant.
    * Alex Lang wrote an insightful [guide](https://www.alexhunterlang.com/nsf-fellowship) for NSF GRFP specifically. It also contains many example pieces submitted by the larger community. 
* [Questions](https://blog.ml.cmu.edu/2020/03/02/questions-to-ask-a-prospective-ph-d-advisor-on-visit-day-with-thorough-and-forthright-explanations/) to ask prospective research advisors. 
* The widely shared Github [repository](https://github.com/shaily99/advice#examples) for Ph.D. + grad school advice.
* For my fellow warriors out there: [How to Prep for Grad School if You're Poor](https://docs.google.com/document/d/1WfoMVkEbsn03Xz0Q2_YQPlWy2YrS0m4R2vK2nsekvcQ/edit). 
* Supporting a fellow Rice Owl: I find [Lucy Lai](https://lucylai.com/blog/gradapps)'s advice to PhD application genuine, helpful, and well-aligned to my personal opinions on a successful PhD application. 

**More related to my area of research...** 
* A [primer on optimal transport](https://vimeo.com/248504509) by Marco Cuturi and Justin Solomon. 
* R&eacute;mi Flamary's wonderful lightning view of [Gromov-Wasserstein for graph learning](https://remi.flamary.com/pres/GW_OTML_2023.pdf). 
* A [goldmine](https://people.ece.cornell.edu/zivg/talks.html) of talks on theoretical properties of Gromov-Wasserstein and Entropic Gromov-Wasserstein space proven by [Ziv](https://people.ece.cornell.edu/zivg/index.html) and his group. Follow him! 
* [Been Kim](https://beenkim.github.io/) has many insightful [talks](https://docs.google.com/presentation/d/e/2PACX-1vT9niyHbIv29umb-p3jNHNvs0wpXwwnXvUdubiunV3J0N8O2E7cTrax4giBfPs-xDVnEdaHdN2Ui9Bd/embed?start=false&loop=false&delayms=60000&slide=id.p) about interpretability, including dangers of misusing this terminology. 
* Many leading researchers spoke about the state of deep learning (2023) in this [piece](https://arxiv.org/abs/2312.09323?fbclid=IwAR12LLw-g_3J9rlxEbTyrk9-JgDRbfl3jFy32V6jhzgeRKUThJgT14FZQlA), with an insightful subsection on interpretability. I especially concur [Zachary Lipton](https://www.zacharylipton.com/)'s statement below:
    > **Zachary Lipton:** Interpretability may be one of the most confused topics in all of machine learning, fraught with confusion and conflict. To begin, the word is badly overloaded. Read an interpretability paper selected at random and you’ll find representations (or insinuations) that the work is addressing “trust”, “insights”, “fairness”, “causality”, “fairness”. Then look at what the authors actually do and you’ll be hard-pressed to tie back the method to any of these underlying motivations. Half the papers produce a set of feature important scores, describing this “importance” in cryptic ways: “what the model is looking at”, “what its internal logic depends on to make this particular prediction”.