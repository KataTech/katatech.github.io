---
layout: page
title: Computational Modeling of Approach-Avoid Task with Reinforcement Learning Frameworks
permalink: /research/bair/
description: connecting AI and developmental psychology...
nav: false
horizontal: false
---

<blockquote>
    Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulate the child's?
    - Alan Turing
</blockquote>

Children are incredible learning agents. Compared to existing AI/ML algorithms, children
- require little supervision and reinforcement
- need very little data
- generalize quickly, and in most cases, reliably
- can form causal relationships 

My time at [Berkeley AI Research](https://bair.berkeley.edu/), specifically the [Cognitive Development and Learning Lab](http://www.gopniklab.berkeley.edu/), focused on inspiring new AI algorithms and models by drawing from children's effective learning process. Specifically, we hone in on the **explore-exploit** trade-off. 

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/research/bair/explore-exploit.png" title="Explore-Exploit Tradeoff" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Robot contemplating between its usual go-to and the new restaurant.
</div>

This refers to the phenomenon where we often have to decide between "trying something new" and "capitalizing on known information" when we make decisions. The explore-exploit trade-off is at the center of learning. One may observe that we, humans, spend a lot time learning and playing with new items as children. As adults, we move into specializations fit for our interests and strengths. In fact, past studies have attributed children's effective learning to their inclination to explore the landscape at a rate much higher than their adult counterparts.

Translating to the realm of AI/ML, this trade-off also plays a role in [Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning) (RL). In particular, many RL training procedure today contain mechanisms to control for when to *explore* new actions as opposed to picking a *known* action with the highest expected long-term reward. 

Our ambition, then, is to study how well existing RL algorithms could *mimick* the actions taken by human (children and adult) participants for an approach-avoid task with reward components. Specificially, we extract best-fit models against experimental data and simulate future reward accumulation based on agents generated by said models.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/research/bair/experimental-design.png" title="Experiment Design" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Experiment Design
</div>

At each iteration of our experiment, participants are given an object with two variables (color and pattern) and they must decide whether to approach it or not. Some objects are "Zaffs" and some are "Non-Zaffs." The difference lies in that "Zaffs" provide positive reward whereas "Non-Zaffs" will lead to penalties.